{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da7fd2a",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All) to avoid typical problems with Jupyter notebooks. **Unfortunately, this does not work with Chrome right now, you will also need to reload the tab in Chrome afterwards**.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". Please put your name here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e70a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Aymane Hachcham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c160ff0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc15fc1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0dab411feddd75be4b23091cf4d0fa8",
     "grade": false,
     "grade_id": "cell-dddd9b472cdda421",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Latent Semantic Indexing\n",
    "\n",
    "First we will implement latent semantic indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43686fed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dd047a5f2cf5fdc2ca134bcc7fecd40",
     "grade": false,
     "grade_id": "cell-cf0a1f8d45f50016",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Load the input data - do not modify\n",
    "import json, gzip, numpy as np\n",
    "raw = json.load(gzip.open(\"/data/simpsonswiki.json.gz\", \"rt\", encoding=\"utf-8\"))\n",
    "titles, texts, classes = [x[\"title\"] for x in raw], [x[\"text\"] for x in raw], [x[\"c\"] for x in raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a878ace3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0265525e0f001028656b86682e2c714c",
     "grade": false,
     "grade_id": "cell-dafc634f8c46b474",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Vectorize the text - do not modify\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "vect = TfidfVectorizer(stop_words=\"english\", sublinear_tf=True, smooth_idf=False, min_df=5)\n",
    "vect.fit(texts)\n",
    "vect.idf_ -= 1\n",
    "idf = vect.idf_\n",
    "tfidf = vect.transform(texts)\n",
    "vocabulary = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc5bdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zorina</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zsa</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuylen</th>\n",
       "      <th>zzyzwicz</th>\n",
       "      <th>zörker</th>\n",
       "      <th>üter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10126 rows × 14153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000   01   02   04   05   06   07   08        10  ...  zoo  zooms  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.087577  ...  0.0    0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  ...  ...    ...   \n",
       "10121  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "10122  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "10123  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "10124  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "10125  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0    0.0   \n",
       "\n",
       "       zorina  zorro  zsa  zuckerberg  zuylen  zzyzwicz  zörker  üter  \n",
       "0         0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "1         0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "2         0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "3         0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "4         0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "...       ...    ...  ...         ...     ...       ...     ...   ...  \n",
       "10121     0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "10122     0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "10123     0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "10124     0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "10125     0.0    0.0  0.0         0.0     0.0       0.0     0.0   0.0  \n",
       "\n",
       "[10126 rows x 14153 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.sparse.from_spmatrix(tfidf, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841eb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7911b9f7fd7af796b2b2e6c41c625839",
     "grade": false,
     "grade_id": "cell-fbcd1050af6399c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implement LSI\n",
    "\n",
    "Implement Latent Semantic Indexing. Do **not** use regular SVD, but instead use truncated SVD from sklearn. (Do not attempt to implement Truncated SVD yourself, use the library here.) Return weights how well the factors explain the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1087ff8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3684f36adea2ec55fd6fb17d909681b3",
     "grade": false,
     "grade_id": "cell-8eb426fb1c1146b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement LSI here\n",
    "def lsi(tfidf, k):\n",
    "    \"\"\"Latent Semantic Indexing. Return the factors, document assignment, and factor weights\"\"\"\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    # We use TruncatedSVD to return the U, Sigma and V_t matrices\n",
    "    # The assignment: => document-topic matrix shape (num_docs, k)\n",
    "    # The factors: => topics-words matrix shape (k, num_words)\n",
    "    # The weights: => The weights for each topic, shape (1, k)\n",
    "    lsi_object = TruncatedSVD(n_components=k, n_iter=100, random_state=42)\n",
    "\n",
    "    assignment = lsi_object.fit_transform(tfidf)\n",
    "    factors = lsi_object.components_\n",
    "    weights = lsi_object.singular_values_\n",
    "\n",
    "    return factors, assignment, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c3c657",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "969bd1e651ce6ee4dd9e120e9fbf631f",
     "grade": true,
     "grade_id": "cell-30066761f95116e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Automatic tests. You do not need to understand or modify this code.\n",
    "_tmp = lsi(tfidf, 2)\n",
    "assert len(_tmp) == 3, \"Incomplete result\"\n",
    "assert _tmp[0].shape == (2, tfidf.shape[1]), \"Factor shape is not correct.\"\n",
    "assert _tmp[1].shape == (tfidf.shape[0], 2), \"Assignment shape is not correct.\"\n",
    "del _tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4af627",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "563e69e91ed535af91bc414ac2ea0bcb",
     "grade": false,
     "grade_id": "cell-f50d41a9220315d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Explore your result\n",
    "\n",
    "Explore the result: write a function to determine the most important words for each factor, and the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dcf971e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d81e82f2aaf8dedf2e57743c005ffe4",
     "grade": false,
     "grade_id": "cell-c3a23922942e74ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def most_important(vocabulary, factor, k=10):\n",
    "    \"\"\"Most important words for each factor\"\"\"\n",
    "    indices_max_values = np.argpartition(factor, -k)[-k:]\n",
    "    list_vocabs = [vocabulary[i] for i in indices_max_values]\n",
    "    return list_vocabs\n",
    "\n",
    "def most_relevant(assignment, k=5):\n",
    "    \"\"\"Most relevant documents for each factor (return document indexes)\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    indices_max_values = np.argpartition(assignment, -k)[-k:]\n",
    "    return indices_max_values\n",
    "\n",
    "def explain(vocabulary, titles=None, classes=None, factors=None, assignment=None, weights=None):\n",
    "    \"\"\"Print an explanation for each factor.\n",
    "       If weights is None, use the relative share of the assignment weights.\n",
    "       Print the ARI when assigning each document to its maximum only.\"\"\"\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    for i, f in enumerate(factors):\n",
    "        print('For the Factor: {}, these are the following results'.format(i))\n",
    "        important_vocabs = most_important(vocabulary, f)\n",
    "        print('The most relevant words in this topic are: ')\n",
    "        print('-------------------------------------------------------')\n",
    "        print('\\n')\n",
    "        print(important_vocabs)\n",
    "        important_docs = most_relevant(assignment)\n",
    "        print('-------------------------------------------------------')\n",
    "        print('\\n')\n",
    "        print('The most relevant documents belonging to this topic are: ')\n",
    "        print([titles[i] for fact in important_docs for i in fact])\n",
    "        print('\\n')\n",
    "        print('Their respective classes are ')\n",
    "        print([classes[i] for fact in important_docs for i in fact])\n",
    "        if weights is not None:\n",
    "            factor_weight = weights[i]\n",
    "            print('-------------------------------------------------------')\n",
    "            print('\\n')\n",
    "            print('The Weight factor for this topic is {}'.format(factor_weight))\n",
    "        print('#################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36f67c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "940359a2f771cf68c33fdf35328e3fd9",
     "grade": true,
     "grade_id": "cell-53f1929d10c5494e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Automatic tests. You do not need to understand or modify this code.\n",
    "_tmp = lsi(tfidf, 2)\n",
    "assert len(most_important(vocabulary, _tmp[0][0], 42)) == 42, \"Wrong number of most important words\"\n",
    "for x in most_important(vocabulary, _tmp[0][0]): assert isinstance(x, str), \"Most important words are not words\"\n",
    "assert len(most_relevant(_tmp[1][:,0], 42)) == 42, \"Wrong number of relevant results.\"\n",
    "from unittest.mock import patch\n",
    "with patch('__main__.most_important') as mock_1, patch('__main__.most_relevant') as mock_2, patch('__main__.print') as mock_3:\n",
    "    explain(vocabulary, titles, classes, *_tmp)\n",
    "    assert mock_1.called, \"You did not use most_important\"\n",
    "    assert mock_2.called, \"You did not use most_central\"\n",
    "    assert mock_3.called, \"You did not print\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8016d412",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f04f83f48c428d2a9c1f7d42f62170c4",
     "grade": false,
     "grade_id": "cell-92ab6cc019fb7fbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Factor: 0, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['maggie', 'song', 'gag', 'family', 'marge', 'lisa', 'homer', 'bart', 'simpson', 'couch']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 11.830191549003196\n",
      "#################################################################\n",
      "For the Factor: 1, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['sit', 'family', 'season', 'appearances', 'plot', 'maggie', 'couch', 'character', 'simpson', 'gag']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 8.415618857292282\n",
      "#################################################################\n",
      "For the Factor: 2, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['got', 'oh', 'love', 'lisa', 'homer', 'performed', 'played', 'sung', 'song', 'lyrics']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 7.03579096328872\n",
      "#################################################################\n",
      "For the Factor: 3, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['father', 'siblings', 'children', 'spuckler', 'school', 'cletus', 'mary', 'bouvier', 'elementary', 'brandine']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 5.477247897329784\n",
      "#################################################################\n",
      "For the Factor: 4, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['writer', 'comedian', 'voiced', 'known', 'starred', 'simpsons', 'born', 'guest', 'american', 'actor']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 5.3855764271852635\n",
      "#################################################################\n",
      "For the Factor: 5, these are the following results\n",
      "The most relevant words in this topic are: \n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "['employee', 'nuclear', 'power', 'great', 'smithers', 'plant', 'burns', 'montgomery', 'mr', 'charles']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The most relevant documents belonging to this topic are: \n",
      "['The Pacifier', 'Bart Jumps', 'Watching Television', 'Babysitting Maggie', 'Good Night', 'Burp Contest', 'Watching Television', 'Bart Jumps', 'Good Night', 'Babysitting Maggie', 'The Pacifier', 'Burp Contest', 'Babysitting Maggie', 'Bart Jumps', 'Watching Television', 'Good Night', 'The Pacifier', 'Burp Contest', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night', 'Watching Television', 'Burp Contest', 'Bart Jumps', 'Babysitting Maggie', 'The Pacifier', 'Good Night']\n",
      "\n",
      "\n",
      "Their respective classes are \n",
      "['Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes', 'Episodes']\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "The Weight factor for this topic is 5.151412141262633\n",
      "#################################################################\n"
     ]
    }
   ],
   "source": [
    "# Explore your result. These should mostly be meaningful topics!\n",
    "lsi_factors, lsi_assignment, lsi_weights = lsi(tfidf, 6)\n",
    "explain(vocabulary, titles, classes, lsi_factors, lsi_assignment, lsi_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c801c7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31e4e93d8c9e9ae7b2adf035f3f049a2",
     "grade": true,
     "grade_id": "cell-cf8643ed125237fa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains additional tests. You do not need to modify this cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
